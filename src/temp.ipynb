{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Using cached langchain-0.3.13-py3-none-any.whl (1.0 MB)\n",
      "Collecting openai\n",
      "  Using cached openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Collecting googletrans\n",
      "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.11.11-cp39-cp39-macosx_10_9_x86_64.whl (468 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.36-cp39-cp39-macosx_10_9_x86_64.whl (2.1 MB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl (184 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Using cached pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17\n",
      "  Downloading langsmith-0.2.6-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.26\n",
      "  Using cached langchain_core-0.3.28-py3-none-any.whl (411 kB)\n",
      "Collecting numpy<2,>=1.22.4\n",
      "  Using cached numpy-1.26.4-cp39-cp39-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.3\n",
      "  Using cached langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Collecting tqdm>4\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Using cached anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.8.2-cp39-cp39-macosx_10_12_x86_64.whl (304 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting googletrans\n",
      "  Downloading googletrans-2.4.0.tar.gz (17 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.18.3-cp39-cp39-macosx_10_9_x86_64.whl (94 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.2.1-cp39-cp39-macosx_10_9_x86_64.whl (46 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.1.0-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (54 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (24.2)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.12-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Using cached pydantic_core-2.27.2-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.1.1-cp39-cp39-macosx_11_0_universal2.whl (270 kB)\n",
      "Using legacy 'setup.py install' for googletrans, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, sniffio, idna, h11, charset-normalizer, certifi, requests, pydantic-core, httpcore, anyio, annotated-types, requests-toolbelt, pydantic, orjson, jsonpointer, httpx, tenacity, PyYAML, propcache, multidict, langsmith, jsonpatch, frozenlist, yarl, langchain-core, greenlet, attrs, async-timeout, aiosignal, aiohappyeyeballs, tqdm, SQLAlchemy, numpy, langchain-text-splitters, jiter, distro, aiohttp, PyPDF2, openai, langchain, googletrans\n",
      "    Running setup.py install for googletrans ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed PyPDF2-3.0.1 PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.7.0 async-timeout-4.0.3 attrs-24.3.0 certifi-2024.12.14 charset-normalizer-3.4.1 distro-1.9.0 frozenlist-1.5.0 googletrans-2.4.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.13 langchain-core-0.3.28 langchain-text-splitters-0.3.4 langsmith-0.2.6 multidict-6.1.0 numpy-1.26.4 openai-1.58.1 orjson-3.10.12 propcache-0.2.1 pydantic-2.10.4 pydantic-core-2.27.2 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.0.0 tqdm-4.67.1 urllib3-2.3.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai PyPDF2 googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (0.3.28)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (6.0.2)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0\n",
      "  Downloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (0.2.6)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.13 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (0.3.13)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.10.4)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (0.3.4)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: anyio in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.7.0)\n",
      "Requirement already satisfied: idna in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.7.0)\n",
      "Collecting python-dotenv>=0.21.0\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
      "Installing collected packages: mypy-extensions, typing-inspect, python-dotenv, marshmallow, pydantic-settings, httpx-sse, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.13 marshmallow-3.23.2 mypy-extensions-1.0.0 pydantic-settings-2.7.0 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/sandeepkumarpalit/Desktop/DATASCIENCE/7_GenAI_Projects/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "from googletrans import Translator\n",
    "\n",
    "# Simple in-memory cache (for demonstration purposes)\n",
    "cache = {}\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_file: Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        A list of Document objects, where each object represents a page of the PDF.\n",
    "    \"\"\"\n",
    "    from PyPDF2 import PdfReader\n",
    "\n",
    "    with open(pdf_file, \"rb\") as f:\n",
    "        reader = PdfReader(f)\n",
    "        documents = [Document(page_content=page.extract_text()) for page in reader.pages]\n",
    "    return documents\n",
    "\n",
    "def translate_text(text):\n",
    "    \"\"\"\n",
    "    Translates the given text to English.\n",
    "\n",
    "    Args:\n",
    "        text: The text to be translated.\n",
    "\n",
    "    Returns:\n",
    "        The translated text.\n",
    "    \"\"\"\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error translating text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def create_vector_store(documents):\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store from a list of documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A list of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        A Chroma vector store.\n",
    "    \"\"\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_api_key:\n",
    "        raise ValueError(\"OpenAI API key not found. Please set the 'OPENAI_API_KEY' environment variable.\")\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    return Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "def get_retrieval_qa_chain(vector_store):\n",
    "    \"\"\"\n",
    "    Creates a RetrievalQA chain with a custom retriever.\n",
    "\n",
    "    Args:\n",
    "        vector_store: A Chroma vector store.\n",
    "\n",
    "    Returns:\n",
    "        A RetrievalQA chain.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(\n",
    "        model_name=\"text-davinci-003\",\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.2,\n",
    "        presence_penalty=0.4\n",
    "    )\n",
    "\n",
    "    with open(\"prompts/summarize_prompt.txt\", \"r\") as prompt_file:\n",
    "        prompt_string = prompt_file.read()\n",
    "\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"query\"],\n",
    "        template=prompt_string\n",
    "    )\n",
    "\n",
    "    def custom_retriever(query, context):\n",
    "        \"\"\"\n",
    "        Retrieves relevant documents from the vector store.\n",
    "\n",
    "        Args:\n",
    "            query: The user's query.\n",
    "            context: Optional context for the search.\n",
    "\n",
    "        Returns:\n",
    "            A list of retrieved documents.\n",
    "        \"\"\"\n",
    "        k_values = [5, 10]\n",
    "        for k in k_values:\n",
    "            retrieved_docs = vector_store.similarity_search(query, n_results=k)\n",
    "            if len(retrieved_docs) >= 3:\n",
    "                return retrieved_docs\n",
    "        return vector_store.similarity_search(query, n_results=len(vector_store.documents))\n",
    "\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=custom_retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt_template}\n",
    "    )\n",
    "\n",
    "def process_pdf(pdf_file, user_query=\"Please summarize the document.\"):\n",
    "    try:\n",
    "        # Step 1: Extract text from the PDF\n",
    "        documents = extract_text_from_pdf(pdf_file)\n",
    "        print(\"Documents Extracted:\", documents)\n",
    "\n",
    "        # Ensure documents are not empty\n",
    "        if not documents or all(not doc.page_content for doc in documents):\n",
    "            return \"No valid text content found in the PDF.\", []\n",
    "\n",
    "        # Step 2: Translate to English\n",
    "        translated_docs = [translate_text(doc.page_content) for doc in documents if doc.page_content]\n",
    "        print(\"Translated Documents:\", translated_docs)\n",
    "\n",
    "        # Ensure we have translated documents\n",
    "        if not translated_docs:\n",
    "            return \"No text could be translated.\", []\n",
    "\n",
    "        # Step 3: Convert translated documents into Document objects\n",
    "        documents_for_store = [Document(page_content=doc) for doc in translated_docs]\n",
    "        print(\"Documents for Store:\", documents_for_store)\n",
    "\n",
    "        # Step 4: Create vector store\n",
    "        cache_key = f\"vector_store_{pdf_file}\"  # Example cache key based on PDF path\n",
    "        if cache_key not in cache:\n",
    "            vector_store = create_vector_store(documents_for_store)\n",
    "            cache[cache_key] = vector_store\n",
    "        else:\n",
    "            vector_store = cache[cache_key]\n",
    "        print(\"Vector Store Created:\", vector_store)\n",
    "\n",
    "        # Step 5: Retrieve relevant documents\n",
    "        relevant_docs = retrieve_relevant_documents(vector_store, user_query) \n",
    "\n",
    "        # Step 6: Create context\n",
    "        context = \" \".join(doc.page_content for doc in relevant_docs) \n",
    "\n",
    "        # Step 7: Get retrieval QA chain\n",
    "        qa_chain = get_retrieval_qa_chain(vector_store)\n",
    "\n",
    "        # Step 8: Generate summary\n",
    "        result = qa_chain({\n",
    "            \"query\": user_query,\n",
    "            \"context\": context\n",
    "        })\n",
    "        print(\"QA Chain Result:\", result)\n",
    "\n",
    "        # Step 9: Return the summarized text and sources\n",
    "        return result[\"result\"], [doc.page_content for doc in result[\"source_documents\"]]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_pdf: {e}\")\n",
    "        return \"An error occurred.\", []\n",
    "\n",
    "# Helper function for retrieving relevant documents\n",
    "def retrieve_relevant_documents(vector_store, query):\n",
    "    k_values = [5, 10]\n",
    "    for k in k_values:\n",
    "        retrieved_docs = vector_store.similarity_search(query, n_results=k)\n",
    "        if len(retrieved_docs) >= 3:\n",
    "            return retrieved_docs\n",
    "    return vector_store.similarity_search(query, n_results=len(vector_store.documents))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"/Users/sandeepkumarpalit/Downloads/Abenteuer_in_der_Sauna.pdf\" \n",
    "    user_query = \"Please summarize the main findings of the research.\" \n",
    "\n",
    "    try:\n",
    "        summary, sources = process_pdf(pdf_file_path, user_query)\n",
    "    except ValueError:\n",
    "        print(\"Error: The process_pdf function returned an unexpected number of values.\")\n",
    "        summary = None\n",
    "        sources = []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        summary = None\n",
    "        sources = []\n",
    "\n",
    "    if summary is not None:\n",
    "        print(\"Summary:\", summary)\n",
    "        print(\"Sources:\", sources)\n",
    "    else:\n",
    "        print(\"Error generating summary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
